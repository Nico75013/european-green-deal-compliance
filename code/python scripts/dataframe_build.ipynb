{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2624a958",
   "metadata": {},
   "source": [
    "This is the **second script to run** in the workflow.  \n",
    "\n",
    "# Build EU Transposition Dataset for Selected Directives\n",
    "\n",
    "This script consolidates national transposition data for a set of Green Deal–related directives into a single cleaned dataset.  \n",
    "\n",
    "**Steps:**  \n",
    "1. Load national transposition tables for selected directives.  \n",
    "2. Keep relevant variables (directive ID, country, dates, amending flag, subject).  \n",
    "3. Concatenate all directives into one dataset.  \n",
    "4. Drop rows with missing/invalid country codes and exclude the UK.  \n",
    "5. Convert date variables to datetime format.  \n",
    "6. Standardize \"Amending\" as a boolean.  \n",
    "7. Remove incoherent rows where publication date is earlier than date of effect.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e945560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Step 1: Load input datasets ===\n",
    "# Load national transposition tables for selected Green Deal-related directives.\n",
    "# ! Replace \"insert/your/path\" with the actual location of your CSV files.\n",
    "df_32017L2001 = pd.read_csv(\"insert/your/path/transposition_32018L2001.csv\")\n",
    "df_32004L0101 = pd.read_csv(\"insert/your/path/transposition_32004L0101.csv\")\n",
    "df_32009L0029 = pd.read_csv(\"insert/your/path/transposition_32009L0029.csv\")\n",
    "df_32010L0031 = pd.read_csv(\"insert/your/path/transposition_32010L0031.csv\")\n",
    "df_32018L0410 = pd.read_csv(\"insert/your/path/transposition_32018L0410.csv\")\n",
    "df_32018L0844 = pd.read_csv(\"insert/your/path/transposition_32018L0844.csv\")\n",
    "df_32018L2002 = pd.read_csv(\"insert/your/path/transposition_32018L2002.csv\")\n",
    "df_32023L0958 = pd.read_csv(\"insert/your/path/transposition_32023L0958.csv\")\n",
    "df_32023L0959 = pd.read_csv(\"insert/your/path/transposition_32023L0959.csv\")\n",
    "df_32023L1791 = pd.read_csv(\"insert/your/path/transposition_32023L1791.csv\")\n",
    "df_32023L2413 = pd.read_csv(\"insert/your/path/transposition_32023L2413.csv\")\n",
    "df_32024L1275 = pd.read_csv(\"insert/your/path/transposition_32024L1275.csv\")\n",
    "df_32009L0028 = pd.read_csv(\"insert/your/path/transposition_32009L0028.csv\")\n",
    "\n",
    "# === Step 2: Select relevant variables ===\n",
    "cols_to_keep = [\n",
    "    \"CELEX\",\n",
    "    \"Directive name\",\n",
    "    \"Date of document\",\n",
    "    \"Date of effect\",\n",
    "    \"Amending\",\n",
    "    \"Country Code\",\n",
    "    \"Country\",\n",
    "    \"Measure Title\",\n",
    "    \"Transposition Deadline\",\n",
    "    \"Publication Date\",\n",
    "    \"Subject matter\"\n",
    "]\n",
    "\n",
    "# Apply selection\n",
    "df_32017L2001_clean = df_32017L2001[cols_to_keep].copy()\n",
    "df_32004L0101_clean = df_32004L0101[cols_to_keep].copy()\n",
    "df_32009L0029_clean = df_32009L0029[cols_to_keep].copy()\n",
    "df_32010L0031_clean = df_32010L0031[cols_to_keep].copy()\n",
    "df_32018L0410_clean = df_32018L0410[cols_to_keep].copy()\n",
    "df_32018L0844_clean = df_32018L0844[cols_to_keep].copy()\n",
    "df_32018L2002_clean = df_32018L2002[cols_to_keep].copy()\n",
    "df_32023L0958_clean = df_32023L0958[cols_to_keep].copy()\n",
    "df_32023L0959_clean = df_32023L0959[cols_to_keep].copy()\n",
    "df_32023L1791_clean = df_32023L1791[cols_to_keep].copy()\n",
    "df_32023L2413_clean = df_32023L2413[cols_to_keep].copy()\n",
    "df_32024L1275_clean = df_32024L1275[cols_to_keep].copy()\n",
    "df_32009L0028_clean = df_32009L0028[cols_to_keep].copy()\n",
    "\n",
    "# === Step 3: Concatenate datasets ===\n",
    "dfs = [\n",
    "    df_32017L2001_clean,\n",
    "    df_32004L0101_clean,\n",
    "    df_32009L0029_clean,\n",
    "    df_32010L0031_clean,\n",
    "    df_32018L0410_clean,\n",
    "    df_32018L0844_clean,\n",
    "    df_32018L2002_clean,\n",
    "    df_32023L0958_clean,\n",
    "    df_32023L0959_clean,\n",
    "    df_32023L1791_clean,\n",
    "    df_32023L2413_clean,\n",
    "    df_32024L1275_clean,\n",
    "    df_32009L0028_clean\n",
    "]\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# === Step 4: Filter by geography (EU only) ===\n",
    "df_all = df_all[\n",
    "    (df_all[\"Country Code\"].notna()) &\n",
    "    (df_all[\"Country Code\"] != \"GBR\") &\n",
    "    (df_all[\"Country Code\"].str.strip() != \"\")\n",
    "]\n",
    "\n",
    "# === Step 5: Convert date variables ===\n",
    "date_cols = [\n",
    "    \"Date of document\",\n",
    "    \"Date of effect\",\n",
    "    \"Transposition Deadline\",\n",
    "    \"Publication Date\",\n",
    "    \"Date of transposition\"\n",
    "]\n",
    "for col in date_cols:\n",
    "    if col in df_all.columns:\n",
    "        df_all[col] = pd.to_datetime(df_all[col], format=\"%m/%d/%y\", errors=\"coerce\")\n",
    "\n",
    "# === Step 6: Clean boolean variable ===\n",
    "df_all[\"Amending\"] = (\n",
    "    df_all[\"Amending\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .map({\"true\": True, \"false\": False, \"yes\": True, \"no\": False, \"1\": True, \"0\": False})\n",
    ")\n",
    "\n",
    "# === Step 7: Detect and drop incoherent rows ===\n",
    "incoherent_dates = df_all[\"Publication Date\"] < df_all[\"Date of effect\"]\n",
    "df_all = df_all[~incoherent_dates]\n",
    "\n",
    "print(\"Final dataset dimensions:\", df_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d3a171",
   "metadata": {},
   "source": [
    "This is the **third script to run** in the workflow.  \n",
    "\n",
    "# Create Dependent Variable: Compliance Time\n",
    "\n",
    "This script calculates the time each Member State took to transpose EU directives, measured as the delay or advance relative to the official deadline.  \n",
    "\n",
    "**Steps:**  \n",
    "1. Compute compliance time as days, weeks, and months between *Transposition Deadline* and *Publication Date*.  \n",
    "2. Check results with a preview of compliance variables.  \n",
    "3. Save the final dataset as `gd_ds_transposition.csv`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d727347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Create dependent variable (compliance time) ===\n",
    "# Calculate time between the transposition deadline and the actual publication date.\n",
    "\n",
    "# Difference in days\n",
    "df_all[\"Compliance_days\"] = (\n",
    "    (df_all[\"Publication Date\"] - df_all[\"Transposition Deadline\"]).dt.days\n",
    ")\n",
    "\n",
    "# Difference in weeks\n",
    "df_all[\"Compliance_weeks\"] = df_all[\"Compliance_days\"] / 7\n",
    "\n",
    "# Difference in months (approximation: 30 days per month)\n",
    "df_all[\"Compliance_months\"] = df_all[\"Compliance_days\"] / 30\n",
    "\n",
    "# === Step 2: Check results ===\n",
    "# Display CELEX, country, deadlines, publication dates, and compliance variables.\n",
    "print(\n",
    "    df_all[\n",
    "        [\n",
    "            \"CELEX\",\n",
    "            \"Country Code\",\n",
    "            \"Transposition Deadline\",\n",
    "            \"Publication Date\",\n",
    "            \"Compliance_days\",\n",
    "            \"Compliance_weeks\",\n",
    "            \"Compliance_months\"\n",
    "        ]\n",
    "    ].head(50)\n",
    ")\n",
    "\n",
    "# === Step 3: Save dataset ===\n",
    "# ! Replace \"insert/your/path\" with your actual folder.\n",
    "df_all.to_csv(\"insert/your/path/gd_ds_transposition.csv\", index=False)\n",
    "print(\"Dataset saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d93f6e",
   "metadata": {},
   "source": [
    "This is the **sixth script to run** in the workflow.  \n",
    "\n",
    "**⚠️ Important prerequisite:**\n",
    "Before running this script, you must have already executed media_salience_rcm.iynp, which generates the **MediaSalience_*.csv** files used here.\n",
    "\n",
    "# Merge Media Salience Data with EU Directive Dataset\n",
    "\n",
    "This script merges **Media Cloud salience** indicators with the dataset of EU Green Deal directives.\n",
    "It attaches media salience measures to each directive based on country and publication year.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d0234",
   "metadata": {},
   "source": [
    "This is the **seventh script to run** in the workflow.  \n",
    "\n",
    "**⚠️ Important prerequisite:**\n",
    "Before running this script, you must have already executed media_salience_rcm.iynp, which generates the **RCM_*.csv** files used here.\n",
    "\n",
    "# Merge RCM Data with EU Directive Dataset\n",
    "\n",
    "This script merges **Relative Climate Salience (RCM)** indicators (from Google Trends) with the dataset of EU Green Deal directives.\n",
    "It attaches RCM values to each directive based on country and publication year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ccac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def merge_rcm_with_transposition(transposition_file, rcm_folder, output_file):\n",
    "    \"\"\"\n",
    "    Merge Relative Climate Salience (RCM) indicators from Google Trends \n",
    "    with the EU directives dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # === 1. Load transposition dataset ===\n",
    "    df_trans = pd.read_csv(transposition_file)\n",
    "\n",
    "    # === 2. Ensure 'Publication Date' is datetime and extract year ===\n",
    "    df_trans['Publication Date'] = pd.to_datetime(df_trans['Publication Date'], errors='coerce')\n",
    "    df_trans['year'] = df_trans['Publication Date'].dt.year\n",
    "\n",
    "    # === 3. Initialize empty RCM columns ===\n",
    "    df_trans['Climate salience (GT)'] = pd.NA\n",
    "    df_trans['Economy salience (GT)'] = pd.NA\n",
    "    df_trans['Relative Climate salience (RCM)'] = pd.NA\n",
    "\n",
    "    # === 4. Loop through each RCM file in the folder ===\n",
    "    for file_name in os.listdir(rcm_folder):\n",
    "        if file_name.startswith(\"RCM_\") and file_name.endswith(\".csv\"):\n",
    "            path = os.path.join(rcm_folder, file_name)\n",
    "            try:\n",
    "                # Load one country’s RCM dataset\n",
    "                df_rcm = pd.read_csv(path)\n",
    "\n",
    "                # Extract country metadata\n",
    "                country_code = df_rcm['Country Code'].iloc[0]\n",
    "                country_name = df_rcm['Country'].iloc[0]\n",
    "\n",
    "                # === 5. Merge RCM data by year and country ===\n",
    "                for idx, row in df_rcm.iterrows():\n",
    "                    mask = (\n",
    "                        (df_trans['Country Code'] == country_code) &\n",
    "                        (df_trans['Country'] == country_name) &\n",
    "                        (df_trans['year'] == row['year'])\n",
    "                    )\n",
    "                    df_trans.loc[mask, 'Climate salience (GT)'] = row['Climate salience (GT)']\n",
    "                    df_trans.loc[mask, 'Economy salience (GT)'] = row['Economy salience (GT)']\n",
    "                    df_trans.loc[mask, 'Relative Climate salience (RCM)'] = row['Relative Climate salience (RCM)']\n",
    "\n",
    "                print(f\"Merged RCM for {country_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {file_name}: {e}\")\n",
    "\n",
    "    # === 6. Save merged dataset ===\n",
    "    df_trans.to_csv(output_file, index=False)\n",
    "    print(f\"Final file saved to: {output_file}\")\n",
    "\n",
    "\n",
    "# === Example usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    #Prerequisite: run rcm_by_country.iynp first to generate RCM_*.csv files\n",
    "    transposition_csv = 'insert/your/path/gd_ds_transposition_with_mediacloud.csv'\n",
    "    rcm_folder = 'insert/your/path/RCM by country'\n",
    "    output_csv = 'insert/your/path/gd_ds_transposition_mediacloud_rcm.csv'\n",
    "\n",
    "    merge_rcm_with_transposition(transposition_csv, rcm_folder, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b7abae",
   "metadata": {},
   "source": [
    "## ⚠️ READ CAREFULLY BELOW ⚠️\n",
    "---\n",
    "### **Prerequisites**  \n",
    "Normally, you would need to run the notebook **`datagov_manifesto.iynb`** to generate the government–manifesto merged dataset.  \n",
    "- ⚠️ The raw **DataGov dataset** was incomplete (many governments were missing).  \n",
    "- These missing governments had to be **added manually**.  \n",
    "- For reproducibility, the cleaned and aggregated dataset `aggregated_governments_with_weighted_avg.csv` has been made available on **GitHub** at:   \n",
    "  ```\n",
    "   --> insert path of github\n",
    "  ```  \n",
    "---\n",
    "\n",
    "This is the **thirtheenth script to run** in the workflow.  \n",
    "\n",
    "# Match EU Directives with Governments in Power  \n",
    "\n",
    "This script matches each **EU Green Deal directive** with the **government in power at the time of its publication**.  \n",
    "It merges the directives dataset (with compliance and salience indicators) with the governments dataset (with weighted averages of party positions).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1. Load datasets ===\n",
    "transposition_df = pd.read_csv('insert/your/path/gd_ds_transposition_mediacloud_rcm.csv')\n",
    "govs_df = pd.read_csv('insert/your/path/aggregated_governments_with_weighted_avg.csv')\n",
    "\n",
    "# === 2. Convert date variables to datetime ===\n",
    "transposition_df[\"Publication Date\"] = pd.to_datetime(transposition_df[\"Publication Date\"], errors=\"coerce\")\n",
    "govs_df[\"Start Date\"] = pd.to_datetime(govs_df[\"Start Date\"], errors=\"coerce\")\n",
    "\n",
    "# === 3. Sort governments by country and start date ===\n",
    "govs_df = govs_df.sort_values(by=[\"Country Code\", \"Start Date\"])\n",
    "\n",
    "# === 4. Function: find government in power at publication date ===\n",
    "def find_government(row, govs):\n",
    "    country = row[\"Country Code\"]\n",
    "    pub_date = row[\"Publication Date\"]\n",
    "\n",
    "    # Filter governments in same country with start date <= publication date\n",
    "    govs_in_country = govs[(govs[\"Country Code\"] == country) & (govs[\"Start Date\"] <= pub_date)]\n",
    "\n",
    "    if not govs_in_country.empty:\n",
    "        # Take the latest (most recent) government\n",
    "        return govs_in_country.iloc[-1]\n",
    "    else:\n",
    "        return pd.Series()  # No government found (edge case)\n",
    "\n",
    "# === 5. Apply function across directives ===\n",
    "matched_governments = transposition_df.apply(lambda row: find_government(row, govs_df), axis=1)\n",
    "\n",
    "# === 6. Combine directive data with matched government attributes ===\n",
    "combined_df = pd.concat([transposition_df.reset_index(drop=True), matched_governments.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# === 7. Save final merged dataset ===\n",
    "combined_df.to_csv('insert/your/path/gd_ds_transposition_with_govs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b933ff0",
   "metadata": {},
   "source": [
    "This is the **fourteenth script to run** in the workflow.  \n",
    "\n",
    "# Add Infringement and Short Names to Directive–Government Dataset  \n",
    "\n",
    "This script prepares the **directives–government merged dataset** for further analysis (e.g., in Stata).  \n",
    "Because Stata has difficulty handling long string variables, the script generates **shortened versions** of measure titles and directive names.  \n",
    "It also flags infringement procedures and extracts references to amended directives.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b94ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === 1. Load dataset ===\n",
    "df = pd.read_csv('insert/your/path/gd_ds_transposition_with_govs.csv')\n",
    "\n",
    "# === 2. Create shortened version of \"Measure Title\" (for Stata compatibility) ===\n",
    "measure_title_short = df[\"Measure Title\"].str.slice(0, 32)\n",
    "\n",
    "# Remove existing column if present (to avoid duplicates)\n",
    "if \"MeasureTitle_short\" in df.columns:\n",
    "    df.drop(columns=[\"MeasureTitle_short\"], inplace=True)\n",
    "\n",
    "# Insert new column right after \"Country\"\n",
    "col_index_country = df.columns.get_loc(\"Country\") + 1\n",
    "df.insert(col_index_country, \"MeasureTitle_short\", measure_title_short)\n",
    "\n",
    "# Drop original \"Measure Title\" column (keep only short version)\n",
    "if \"Measure Title\" in df.columns:\n",
    "    df.drop(columns=[\"Measure Title\"], inplace=True)\n",
    "\n",
    "# === 3. Create infringement-related variables ===\n",
    "# Flag: True if measure title starts with \"INFR\"\n",
    "infringement_procedure = measure_title_short.fillna(\"\").str.startswith(\"INFR\")\n",
    "\n",
    "# Extract infringement name if flag is True\n",
    "infringement_name = measure_title_short.where(infringement_procedure)\n",
    "\n",
    "# Remove old columns if present\n",
    "for col in [\"Infringement_Procedure\", \"Infringement_Name\"]:\n",
    "    if col in df.columns:\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# Insert new infringement columns right after \"Compliance_months\"\n",
    "col_index_comp = df.columns.get_loc(\"Compliance_months\") + 1\n",
    "df.insert(col_index_comp, \"Infringement_Procedure\", infringement_procedure.astype(bool))\n",
    "df.insert(col_index_comp + 1, \"Infringement_Name\", infringement_name)\n",
    "\n",
    "# === 4. Create short directive names ===\n",
    "def extract_amended(text, amending_flag):\n",
    "    \"\"\"\n",
    "    Extracts the name of the amended directive if 'Amending' is True\n",
    "    and the directive name contains an 'amending Directive ...' clause.\n",
    "    \"\"\"\n",
    "    if not amending_flag or pd.isna(text):\n",
    "        return \"\"\n",
    "    match = re.search(r\"amending\\s+(Directive\\s+[A-Z]*\\s*\\d+/\\d+)\", text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Extract base directive name (before \" of ...\")\n",
    "df[\"DirectiveName_short\"] = df[\"Directive name\"].str.split(\" of \", n=1).str[0]\n",
    "\n",
    "# Extract amended directive reference if applicable\n",
    "df[\"AmendedName_short\"] = [\n",
    "    extract_amended(txt, amend) for txt, amend in zip(df[\"Directive name\"], df[\"Amending\"])\n",
    "]\n",
    "\n",
    "# Insert short directive names right after \"Directive name\"\n",
    "col_index_dir = df.columns.get_loc(\"Directive name\") + 1\n",
    "df.insert(col_index_dir, \"DirectiveName_short\", df.pop(\"DirectiveName_short\"))\n",
    "df.insert(col_index_dir + 1, \"AmendedName_short\", df.pop(\"AmendedName_short\"))\n",
    "\n",
    "# Drop original \"Directive name\"\n",
    "if \"Directive name\" in df.columns:\n",
    "    df.drop(columns=[\"Directive name\"], inplace=True)\n",
    "\n",
    "# === 5. Save updated dataset (UTF-8 BOM for Stata compatibility) ===\n",
    "output_path = 'insert/your/path/gd_ds_transposition_with_govs_infr.csv'\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"File correctly saved in: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f71baeb",
   "metadata": {},
   "source": [
    "This is the **last script to run** in the workflow. \n",
    "\n",
    "--- \n",
    "## Prerequisites \n",
    "\n",
    "Before running this script, you must first run:  \n",
    "**`EB_merger.iynb`** → generates the harmonised and merged Eurobarometer CRP dataset (`EB_CRPs_interp.csv`).  \n",
    "\n",
    "---\n",
    "# Merge Directives Dataset with Eurobarometer CRP  \n",
    "\n",
    "This script merges the **EU directives–government dataset** with the **Eurobarometer climate risk perception (CRP) panel**.  \n",
    "\n",
    "The output is saved as an **Excel file** (`.xlsx`), ready to be imported into **Stata**.  \n",
    "Excel format was chosen because saving as `.csv` caused many numeric variables to be incorrectly recorded as strings, creating issues for analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488fee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === File paths ===\n",
    "transposition_csv = \"insert/your/path/gd_ds_transposition_with_govs_infr.csv\"\n",
    "eb_crps_csv = \"insert/your/path/EB_CRPs_interp.csv\"\n",
    "output_csv = \"insert/your/path/gd_ds_transposition_for_stata.csv\"\n",
    "\n",
    "\n",
    "def merge_transposition_with_ebcrps(transposition_file, eb_file, output_file):\n",
    "    \"\"\"\n",
    "    Merge EU directives–government dataset with interpolated\n",
    "    Eurobarometer climate risk perception (CRP) values and\n",
    "    save the result as an Excel file.\n",
    "    \"\"\"\n",
    "\n",
    "    # === 1. Load datasets ===\n",
    "    df_trans = pd.read_csv(transposition_file)\n",
    "    df_eb = pd.read_csv(eb_file)\n",
    "\n",
    "    # === 2. Ensure 'year' exists in transposition data ===\n",
    "    if \"year\" not in df_trans.columns and \"Publication Date\" in df_trans.columns:\n",
    "        df_trans[\"Publication Date\"] = pd.to_datetime(df_trans[\"Publication Date\"], errors=\"coerce\")\n",
    "        df_trans[\"year\"] = df_trans[\"Publication Date\"].dt.year\n",
    "\n",
    "    # === 3. Merge on country and year ===\n",
    "    df_merged = df_trans.merge(\n",
    "        df_eb,\n",
    "        left_on=[\"Country\", \"year\"],\n",
    "        right_on=[\"country\", \"year\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # === 4. Drop duplicate 'country' column from EB_CRPs ===\n",
    "    if \"country\" in df_merged.columns:\n",
    "        df_merged.drop(columns=[\"country\"], inplace=True)\n",
    "\n",
    "    # === 5. Check for missing matches ===\n",
    "    missing = df_merged[df_merged[\"crp_raw\"].isna()]\n",
    "    print(f\" Missing matches: {len(missing)} out of {len(df_merged)} rows\")\n",
    "\n",
    "    # === 6. Save merged dataset as Excel ===\n",
    "    df_merged.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "    print(f\"Final file saved to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
