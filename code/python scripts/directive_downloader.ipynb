{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9021b40",
   "metadata": {},
   "source": [
    "This is the **firts script to run** in the workflow.  \n",
    "\n",
    "# Scrape EUR-Lex Transposition Data for a Directive\n",
    "\n",
    "This script downloads and processes **National Implementing Measures (NIMs)** for a specific EU directive from EUR-Lex.  \n",
    "It extracts both **directive-level metadata** (from the `ALL` page) and **country-level measures** (from the `NIM` page), and produces a standardized CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# URL of the directive’s NIM (National Implementing Measures) page on EUR-Lex.\n",
    "# Example provided: Directive 32003L0096. Replace with another NIM link if needed.\n",
    "eurlex_url = 'https://eur-lex.europa.eu/legal-content/EN/NIM/?uri=CELEX:32003L0096'  \n",
    "\n",
    "# Whether the directive is an amendment (set as a flag in the dataset).\n",
    "ammend = False   \n",
    "\n",
    "# Local directory where the final CSV will be saved.\n",
    "output_dir = 'insert/your/path'  # ! Replace with your destination folder.\n",
    "\n",
    "# === UTILITY FUNCTIONS ===\n",
    "def standardize_date(date_str):\n",
    "    \"\"\"\n",
    "    Try to standardize dates into YYYY-MM-DD format.\n",
    "    - Handles both DD/MM/YYYY and YYYY-MM-DD formats.\n",
    "    - Returns an empty string if no valid format is found.\n",
    "    \"\"\"\n",
    "    if not date_str: return \"\"\n",
    "    date_str = date_str.strip()\n",
    "    match = re.match(r\"^(\\d{2})/(\\d{2})/(\\d{4})$\", date_str)  # format: DD/MM/YYYY\n",
    "    if match:\n",
    "        day, month, year = match.groups()\n",
    "        return f\"{year}-{month}-{day}\"\n",
    "    match = re.match(r\"^(\\d{4})-(\\d{2})-(\\d{2})$\", date_str)  # format: YYYY-MM-DD\n",
    "    if match:\n",
    "        return date_str\n",
    "    return date_str\n",
    "\n",
    "def get_celex_from_url(url):\n",
    "    \"\"\"\n",
    "    Extract the CELEX number from a EUR-Lex URL.\n",
    "    Example: \"...CELEX:32003L0096\" → \"32003L0096\".\n",
    "    \"\"\"\n",
    "    m = re.search(r'CELEX:([0-9A-Z]+)', url, re.I)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def extract_dl_metadata(soup):\n",
    "    \"\"\"\n",
    "    Parse the ALL page of a directive to extract directive-level metadata.\n",
    "    Metadata on EUR-Lex is typically stored in <dl class=\"NMetadata\"> blocks.\n",
    "    - Collects <dt> labels and their corresponding <dd> values.\n",
    "    - Prefers <span> text when available (cleaner).\n",
    "    - Handles multiple values for \"Date of transposition\".\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    multi_label_fields = {\"Date of transposition\"}  # these can appear multiple times\n",
    "\n",
    "    for dl in soup.find_all(\"dl\", class_=\"NMetadata\"):\n",
    "        dts = dl.find_all(\"dt\")\n",
    "        dds = dl.find_all(\"dd\")\n",
    "        for dt, dd in zip(dts, dds):\n",
    "            label = dt.get_text(strip=True).replace(\":\", \"\")\n",
    "            spans = dd.find_all(\"span\")\n",
    "            if spans:\n",
    "                # Multiple <span> values → join them\n",
    "                value = \", \".join(span.get_text(strip=True) for span in spans)\n",
    "            else:\n",
    "                value = dd.get_text(\" \", strip=True)\n",
    "            # Keep only the first chunk if separated by \";\"\n",
    "            value = value.split(\";\")[0].strip()\n",
    "            if label in multi_label_fields:\n",
    "                # Append additional values if they exist\n",
    "                if label in metadata:\n",
    "                    if value not in metadata[label].split(\", \"):\n",
    "                        metadata[label] += \", \" + value\n",
    "                else:\n",
    "                    metadata[label] = value\n",
    "            else:\n",
    "                # Only keep the first occurrence for unique fields\n",
    "                if label not in metadata:\n",
    "                    metadata[label] = value\n",
    "    return metadata\n",
    "\n",
    "# === 1. DOWNLOAD NIM + ALL PAGES ===\n",
    "# Extract CELEX code from URL for naming outputs\n",
    "celex = get_celex_from_url(eurlex_url)\n",
    "\n",
    "# Define NIM (transposition measures) and ALL (directive metadata) URLs\n",
    "nim_url = eurlex_url\n",
    "all_url = eurlex_url.replace('/NIM/', '/ALL/')\n",
    "\n",
    "# Set headers to mimic a browser (avoid blocking by EUR-Lex)\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "# Download the NIM page (country-level transposition info)\n",
    "nim_response = requests.get(nim_url, headers=headers)\n",
    "nim_html = nim_response.text\n",
    "nim_soup = BeautifulSoup(nim_html, \"html.parser\")\n",
    "\n",
    "# Download the ALL page (general directive metadata)\n",
    "all_response = requests.get(all_url, headers=headers)\n",
    "all_html = all_response.text\n",
    "all_soup = BeautifulSoup(all_html, \"html.parser\")\n",
    "\n",
    "# === 2. EXTRACT METADATA FROM ALL PAGE ===\n",
    "# Collect directive-level metadata (applies to all countries).\n",
    "all_metadata = extract_dl_metadata(all_soup)\n",
    "print(\"Extracted document metadata:\")\n",
    "for k, v in all_metadata.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# === 3. EXTRACT NATIONAL MEASURES FROM NIM PAGE ===\n",
    "# List of EU countries (ISO-3 codes → country names).\n",
    "# UK is excluded since no longer part of the EU.\n",
    "eu_countries = {\n",
    "    \"AUT\": \"Austria\", \"BEL\": \"Belgium\", \"BGR\": \"Bulgaria\", \"HRV\": \"Croatia\",\n",
    "    \"CYP\": \"Cyprus\", \"CZE\": \"Czechia\", \"DNK\": \"Denmark\", \"EST\": \"Estonia\",\n",
    "    \"FIN\": \"Finland\", \"FRA\": \"France\", \"DEU\": \"Germany\", \"GRC\": \"Greece\",\n",
    "    \"HUN\": \"Hungary\", \"IRL\": \"Ireland\", \"ITA\": \"Italy\", \"LVA\": \"Latvia\",\n",
    "    \"LTU\": \"Lithuania\", \"LUX\": \"Luxembourg\", \"MLT\": \"Malta\", \"NLD\": \"Netherlands\",\n",
    "    \"POL\": \"Poland\", \"PRT\": \"Portugal\", \"ROU\": \"Romania\", \"SVK\": \"Slovakia\",\n",
    "    \"SVN\": \"Slovenia\", \"ESP\": \"Spain\", \"SWE\": \"Sweden\"\n",
    "}\n",
    "\n",
    "data = []  # list of rows that will form the dataset\n",
    "# Each <li> with class ending in \"_ntm\" corresponds to a national transposition measure\n",
    "li_list = nim_soup.find_all(\"li\", class_=lambda x: x and x.endswith(\"_ntm\"))\n",
    "found_country_codes = set()  # to track countries that have measures\n",
    "\n",
    "for li in li_list:\n",
    "    # Extract country code (e.g., \"FRA\" from \"FRA_ntm\") and map to country name\n",
    "    li_classes = li.get(\"class\")\n",
    "    country_code = [c.split(\"_\")[0] for c in li_classes if c.endswith(\"_ntm\")][0]\n",
    "    country_name = eu_countries.get(country_code, country_code)\n",
    "    found_country_codes.add(country_code)\n",
    "\n",
    "    # --- Extract transposition deadline ---\n",
    "    # The deadline may appear in different places, so try several strategies.\n",
    "    deadline = None\n",
    "    search_node = li\n",
    "    while search_node:\n",
    "        prev = search_node.previous_sibling\n",
    "        found = False\n",
    "        while prev:\n",
    "            # Look in preceding <div> with class containing \"transposition\"\n",
    "            if getattr(prev, \"name\", None) == \"div\" and \"transposition\" in prev.get(\"class\", []):\n",
    "                m = re.search(r\"Transposition deadline[s]*:\\s*([0-9/, ]+)\", prev.get_text())\n",
    "                if m:\n",
    "                    deadline = m.group(1).strip()\n",
    "                    found = True\n",
    "                    break\n",
    "            prev = prev.previous_sibling\n",
    "        if found:\n",
    "            break\n",
    "        # Move upward in the DOM if not found\n",
    "        search_node = search_node.parent\n",
    "        if getattr(search_node, \"get\", lambda x: None)(\"id\") == country_code:\n",
    "            break\n",
    "\n",
    "    # Alternative location: inside country panel\n",
    "    if not deadline:\n",
    "        parent_panel = li.find_parent(\"div\", class_=\"panel-collapse countryPanel\")\n",
    "        if parent_panel:\n",
    "            deadline_div = parent_panel.find(\"div\", id=f\"{country_code}_transp\")\n",
    "            if deadline_div:\n",
    "                m = re.search(r\"Transposition deadline[s]*:\\s*([0-9/, ]+)\", deadline_div.get_text())\n",
    "                if m:\n",
    "                    deadline = m.group(1).split(\",\")[0].strip()\n",
    "    # Last fallback: inside generic row section\n",
    "    if not deadline:\n",
    "        row_div = nim_soup.find(\"div\", class_=\"col-sm-12 ntmRow\")\n",
    "        if row_div:\n",
    "            deadline_div = row_div.find(\"div\", id=f\"{country_code}_transposition\")\n",
    "            if deadline_div:\n",
    "                deadline = deadline_div.get_text(strip=True).split(\",\")[0].strip()\n",
    "\n",
    "    # --- Extract measure title (name of national implementing act) ---\n",
    "    title_tag = li.find(\"a\", id=\"titleLink\")\n",
    "    measure_title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "    # --- Extract publication date ---\n",
    "    publication_date = None\n",
    "    pub_txt = li.get_text(\" \", strip=True)\n",
    "    m = re.search(r'Publication date:\\s*([\\d-]+)', pub_txt)\n",
    "    if m:\n",
    "        publication_date = standardize_date(m.group(1))\n",
    "    else:\n",
    "        # Sometimes stored in <em> tags\n",
    "        for em in li.find_all(\"em\"):\n",
    "            if \"Publication date\" in em.parent.text:\n",
    "                publication_date = standardize_date(em.text.strip())\n",
    "\n",
    "    # --- Standardize deadline (multiple deadlines → join) ---\n",
    "    deadline_std = \"\"\n",
    "    if deadline:\n",
    "        deadlines = [standardize_date(x) for x in deadline.split(\",\")]\n",
    "        deadline_std = \", \".join(deadlines)\n",
    "\n",
    "    # --- Append row to dataset if measure exists ---\n",
    "    if measure_title:\n",
    "        row = {\n",
    "            \"CELEX\": celex,\n",
    "            \"Country Code\": country_code,\n",
    "            \"Country\": country_name,\n",
    "            \"Transposition Deadline\": deadline_std,\n",
    "            \"Measure Title\": measure_title,\n",
    "            \"Publication Date\": publication_date,\n",
    "            \"Amending\": ammend,\n",
    "        }\n",
    "        # Add directive-level metadata to row\n",
    "        row.update(all_metadata)\n",
    "        data.append(row)\n",
    "\n",
    "# === 4. ADD COUNTRIES WITH NO MEASURES ===\n",
    "# For countries with no entry on the NIM page, add an empty row with metadata.\n",
    "for code, name in eu_countries.items():\n",
    "    if code not in found_country_codes:\n",
    "        row = {\n",
    "            \"CELEX\": celex,\n",
    "            \"Country Code\": code,\n",
    "            \"Country\": name,\n",
    "            \"Transposition Deadline\": \"\",\n",
    "            \"Measure Title\": \"\",\n",
    "            \"Publication Date\": \"\",\n",
    "            \"Amending\": ammend,\n",
    "        }\n",
    "        row.update(all_metadata)\n",
    "        data.append(row)\n",
    "\n",
    "# === 5. SAVE DATASET TO CSV ===\n",
    "# Convert to DataFrame, sort for consistency, and save to disk.\n",
    "df = pd.DataFrame(data)\n",
    "df.sort_values(by=[\"Country Code\", \"Transposition Deadline\", \"Measure Title\"], inplace=True)\n",
    "\n",
    "output_filename = f\"transposition_{celex}.csv\"\n",
    "output_path = os.path.join(output_dir, output_filename)\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved to {output_path}\")\n",
    "print(df.head(15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
